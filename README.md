# AWS ParallelCluster for Distributed Training

![Architecture Diagram](img/architecture.png)

AWS ParallelClusterë¥¼ ì‚¬ìš©í•œ ë¶„ì‚° í•™ìŠµ í™˜ê²½ êµ¬ì¶• ì†”ë£¨ì…˜ì…ë‹ˆë‹¤. XPU ì¸ìŠ¤í„´ìŠ¤ (ì˜ˆ: p6-b200.48xlarge with B200 GPUs)ì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë©°, ëª¨ë‹ˆí„°ë§ ìŠ¤íƒê³¼ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

## ğŸ—ï¸ Architecture Overview

### ì£¼ìš” êµ¬ì„± ìš”ì†Œ

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Application Load    â”‚
                    â”‚     Balancer         â”‚
                    â”‚  (Optional HTTPS)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â”‚ Port 443/80
                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              â”‚                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚   Monitoring Instance             â”‚                      â”‚
â”‚  â”‚   (t3.medium - Standalone)        â”‚                      â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                      â”‚
â”‚  â”‚ â€¢ Prometheus                      â”‚                      â”‚
â”‚  â”‚ â€¢ Grafana :3000                   â”‚                      â”‚
â”‚  â”‚ â€¢ Persistent Storage              â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚      LoginNode Pool               â”‚                      â”‚
â”‚  â”‚      (m5.large x2)                â”‚                      â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                      â”‚
â”‚  â”‚ â€¢ User Access (SSH)               â”‚                      â”‚
â”‚  â”‚ â€¢ Job Submission                  â”‚                      â”‚
â”‚  â”‚ â€¢ Data Preprocessing              â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                  â”‚                                           â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚       â”‚   HeadNode          â”‚    â”‚   ComputeNodes          â”‚â”‚
â”‚       â”‚   (m5.8xlarge)      â”‚    â”‚   (p6-b200.48xlarge)    â”‚â”‚
â”‚       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚       â”‚ â€¢ Slurm Master      â”‚    â”‚ â€¢ 8x B200 GPUs (192GB)  â”‚â”‚
â”‚       â”‚ â€¢ Job Scheduler     â”‚    â”‚ â€¢ 192 vCPUs, 2TB RAM    â”‚â”‚
â”‚       â”‚ â€¢ NFS Server        â”‚    â”‚ â€¢ 3.2Tbps Network       â”‚â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                  â”‚                           â”‚               â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                              â”‚                               â”‚
â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚                 â”‚   Shared Storage        â”‚                  â”‚
â”‚                 â”‚   (FSx Lustre)          â”‚                  â”‚
â”‚                 â”‚   â€¢ High-performance    â”‚                  â”‚
â”‚                 â”‚   â€¢ Multi-GB/s          â”‚                  â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        VPC (10.0.0.0/16)
```

### ë…¸ë“œ ì—­í• 

- **Monitoring Instance (ë…ë¦½í˜•)**: 
  - í´ëŸ¬ìŠ¤í„°ì™€ ë³„ë„ë¡œ ìš´ì˜ë˜ëŠ” ëª¨ë‹ˆí„°ë§ ì „ìš© ì„œë²„
  - í´ëŸ¬ìŠ¤í„° ì‚­ì œ ì‹œì—ë„ ëª¨ë‹ˆí„°ë§ ë°ì´í„° ìœ ì§€
  - ALBë¥¼ í†µí•œ ì•ˆì „í•œ ì›¹ ì ‘ê·¼ (ì§ì ‘ Public IP ë…¸ì¶œ ë°©ì§€)
  
- **LoginNode Pool**: 
  - ì‚¬ìš©ì ì ‘ê·¼ ë° ì‘ì—… ì œì¶œ ì „ìš©
  - ë°ì´í„° ì „ì²˜ë¦¬ ë° ê°„ë‹¨í•œ ì‘ì—… ìˆ˜í–‰
  - HeadNodeì˜ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ ë³´í˜¸
  
- **HeadNode**: 
  - Slurm ìŠ¤ì¼€ì¤„ëŸ¬ ë° ì‘ì—… ê´€ë¦¬
  - Private Subnetì— ìœ„ì¹˜ (ë³´ì•ˆ)
  - NFS ì„œë²„ ì—­í• 
  
- **ComputeNodes**: 
  - GPU ì›Œí¬ë¡œë“œ ì‹¤í–‰ ì „ìš© (ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ë³„ ì„¤ì •: [ê°€ì´ë“œ](guide/INSTANCE-TYPE-CONFIGURATION.md))
  - Private Subnetì— ìœ„ì¹˜
  - Auto-scaling ì§€ì›

ğŸ“– **ìƒì„¸ ì•„í‚¤í…ì²˜ ì„¤ëª…**: [guide/ARCHITECTURE.md](guide/ARCHITECTURE.md)

## ğŸ“ Directory Structure

```
.
â”œâ”€â”€ README.md                                    # ì´ íŒŒì¼
â”œâ”€â”€ guide/                                       # ìƒì„¸ ê°€ì´ë“œ ë¬¸ì„œ
â”‚   â”œâ”€â”€ ARCHITECTURE.md                          # ì•„í‚¤í…ì²˜ ìƒì„¸ ì„¤ëª…
â”‚   â”œâ”€â”€ CONFIGURATION.md                         # í´ëŸ¬ìŠ¤í„° ì„¤ì • ê°€ì´ë“œ
â”‚   â”œâ”€â”€ INSTALLATION.md                          # ì„¤ì¹˜ ê°€ì´ë“œ
â”‚   â”œâ”€â”€ MONITORING.md                            # ëª¨ë‹ˆí„°ë§ ì„¤ì •
â”‚   â”œâ”€â”€ SECURITY.md                              # ë³´ì•ˆ ê°€ì´ë“œ
â”‚   â””â”€â”€ TROUBLESHOOTING.md                       # ë¬¸ì œ í•´ê²°
â”‚
â”œâ”€â”€ parallelcluster-infrastructure.yaml          # CloudFormation ì¸í”„ë¼ í…œí”Œë¦¿
â”œâ”€â”€ cluster-config.yaml.template                 # í´ëŸ¬ìŠ¤í„° ì„¤ì • í…œí”Œë¦¿
â”œâ”€â”€ environment-variables.sh                     # í™˜ê²½ ë³€ìˆ˜ í…œí”Œë¦¿
â”‚
â”œâ”€â”€ config/                                      # ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ monitoring/                              # ëª¨ë‹ˆí„°ë§ ì¸ìŠ¤í„´ìŠ¤ (ì°¸ê³ ìš©)
â”‚   â”‚   â”œâ”€â”€ README.md                            # âš ï¸ UserData ìë™ ì„¤ì¹˜ ë°©ì‹ ì„¤ëª…
â”‚   â”‚   â””â”€â”€ setup-monitoring-instance.sh         # ìˆ˜ë™ ì¬ì„¤ì¹˜ìš© (ì°¸ê³ )
â”‚   â”œâ”€â”€ headnode/                                # HeadNode ì„¤ì •
â”‚   â”œâ”€â”€ loginnode/                               # LoginNode ì„¤ì •
â”‚   â”œâ”€â”€ compute/                                 # ComputeNode ì„¤ì •
â”‚   â””â”€â”€ cloudwatch/                              # CloudWatch ì„¤ì •
â”‚
â”œâ”€â”€ scripts/                                     # ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ (S3 ì—…ë¡œë“œìš©)
â”‚   â”œâ”€â”€ nccl/                                    # NCCL ì„¤ì¹˜ ë° í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ efa/                                     # EFA ë“œë¼ì´ë²„
â”‚   â”œâ”€â”€ cloudwatch/                              # CloudWatch ì—ì´ì „íŠ¸
â”‚   â””â”€â”€ shared-storage/                          # ê³µìœ  ìŠ¤í† ë¦¬ì§€ ì„¤ì •
â”‚
â””â”€â”€ tests/                                       # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    â””â”€â”€ nccl/                                    # NCCL ë²¤ì¹˜ë§ˆí¬
```

## ğŸ“¦ Prerequisites

```bash
# AWS CLI v2
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip && sudo ./aws/install

# AWS ParallelCluster CLI v3.14.0 in virtual environment
python3 -m venv pcluster-venv
source pcluster-venv/bin/activate
pip install --upgrade "aws-parallelcluster==3.14.0"

# envsubst (í…œí”Œë¦¿ ë³€ìˆ˜ ì¹˜í™˜)
# MacOS
curl -L https://github.com/a8m/envsubst/releases/download/v1.2.0/envsubst-`uname -s`-`uname -m` -o envsubst
chmod +x envsubst && sudo mv envsubst /usr/local/bin

# Linux (CloudShellì—ëŠ” ê¸°ë³¸ ì„¤ì¹˜ë¨)
sudo yum install -y gettext  # Amazon Linux
# sudo apt-get install -y gettext-base  # Ubuntu

# AWS ìê²© ì¦ëª… ì„¤ì •
# regionì€ í´ëŸ¬ìŠ¤í„°ë¥¼ ë°°í¬í•  ë¦¬ì „ê³¼ ì¼ì¹˜í•´ì•¼í•¨, cluster-config.yaml íŒŒì¼ì—ì„œ ì°¸ì¡°í•¨
aws configure
```

## ğŸš€ Quick Start

### 1. ì¸í”„ë¼ ë°°í¬

```bash
# í˜„ì¬ IP í™•ì¸
MY_IP=$(curl -s https://checkip.amazonaws.com)
echo "Your IP: $MY_IP"

# ê¸°ë³¸ ë°°í¬ (ALB ì—†ìŒ)
REGION="us-east-2"
aws cloudformation create-stack \
  --stack-name parallelcluster-infra \
  --region $REGION \
  --template-body file://parallelcluster-infrastructure.yaml \
  --parameters \
    ParameterKey=PrimarySubnetAZ,ParameterValue=${REGION}a \
    ParameterKey=MonitoringType,ParameterValue=none \
  --capabilities CAPABILITY_IAM

# Self-hosted monitoring with ALB (ê¶Œì¥)
aws cloudformation create-stack \
  --stack-name parallelcluster-infra \
  --region $REGION \
  --template-body file://parallelcluster-infrastructure.yaml \
  --parameters \
    ParameterKey=PrimarySubnetAZ,ParameterValue=${REGION}a \
    ParameterKey=MonitoringType,ParameterValue=self-hosting \
    ParameterKey=SecondarySubnetAZ,ParameterValue=${REGION}b \
    ParameterKey=S3BucketName,ParameterValue=my-pcluster-scripts \
    ParameterKey=MonitoringKeyPair,ParameterValue=your-key \
    ParameterKey=AllowedIPsForMonitoringSSH,ParameterValue="${MY_IP}/32" \
    ParameterKey=AllowedIPsForALB,ParameterValue="${MY_IP}/32" \
  --capabilities CAPABILITY_IAM

# AWS Managed Prometheus (AMP) ì‚¬ìš© (ìë™ ìƒì„±)
aws cloudformation create-stack \
  --stack-name parallelcluster-infra \
  --region $REGION \
  --template-body file://parallelcluster-infrastructure.yaml \
  --parameters \
    ParameterKey=PrimarySubnetAZ,ParameterValue=${REGION}a \
    ParameterKey=MonitoringType,ParameterValue=amp \
  --capabilities CAPABILITY_IAM

# AMP Workspace ì •ë³´ í™•ì¸
AMP_WORKSPACE_ID=$(aws cloudformation describe-stacks \
  --stack-name parallelcluster-infra \
  --query 'Stacks[0].Outputs[?OutputKey==`AMPWorkspaceId`].OutputValue' \
  --output text)

AMP_ENDPOINT=$(aws cloudformation describe-stacks \
  --stack-name parallelcluster-infra \
  --query 'Stacks[0].Outputs[?OutputKey==`AMPPrometheusEndpoint`].OutputValue' \
  --output text)

echo "AMP Workspace ID: $AMP_WORKSPACE_ID"
echo "AMP Endpoint: $AMP_ENDPOINT"

# âš ï¸ ì°¸ê³ : AMP Endpointë¥¼ ë¸Œë¼ìš°ì €ë¡œ ì ‘ê·¼í•˜ë©´ <HttpNotFoundException/>ê°€ í‘œì‹œë©ë‹ˆë‹¤.
# ì´ëŠ” ì •ìƒ ë™ì‘ì…ë‹ˆë‹¤! AMPëŠ” Prometheus remote_write APIë§Œ ì œê³µí•˜ë©°,
# ë©”íŠ¸ë¦­ ì¡°íšŒëŠ” Grafanaë¥¼ í†µí•´ì„œë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.

# AMP Workspace ìƒíƒœ í™•ì¸ (ACTIVEì—¬ì•¼ ì •ìƒ)
aws amp describe-workspace --workspace-id $AMP_WORKSPACE_ID \
  --query 'workspace.status.statusCode' --output text

# ì™„ì „ ê´€ë¦¬í˜• ëª¨ë‹ˆí„°ë§ ë°°í¬ (AMP + AMG, ê¶Œì¥)
aws cloudformation create-stack \
  --stack-name parallelcluster-infra \
  --region $REGION \
  --template-body file://parallelcluster-infrastructure.yaml \
  --parameters \
    ParameterKey=PrimarySubnetAZ,ParameterValue=${REGION}a \
    ParameterKey=MonitoringType,ParameterValue=amp+amg \
  --capabilities CAPABILITY_NAMED_IAM

# ëª¨ë‹ˆí„°ë§ ì—†ì´ ë°°í¬ (ìµœì†Œ ì„¤ì •)
aws cloudformation create-stack \
  --stack-name parallelcluster-infra \
  --region $REGION \
  --template-body file://parallelcluster-infrastructure.yaml \
  --parameters \
    ParameterKey=PrimarySubnetAZ,ParameterValue=${REGION}a \
    ParameterKey=MonitoringType,ParameterValue=none \
  --capabilities CAPABILITY_IAM

# ë°°í¬ ì™„ë£Œ ëŒ€ê¸° (~5-8ë¶„)
aws cloudformation wait stack-create-complete \
  --stack-name parallelcluster-infra \
  --region $REGION
```

### 2. S3 ë²„í‚· ë° ìŠ¤í¬ë¦½íŠ¸ ì—…ë¡œë“œ

```bash
# S3 ë²„í‚· ìƒì„±
aws s3 mb s3://my-pcluster-scripts --region us-east-2

# ìŠ¤í¬ë¦½íŠ¸ ì—…ë¡œë“œ
aws s3 sync scripts/ s3://my-pcluster-scripts/scripts/ --region us-east-2
```

### 3. í´ëŸ¬ìŠ¤í„° ì„¤ì • ìƒì„±

```bash
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
vim environment-variables.sh
# í•„ìˆ˜ ìˆ˜ì • í•­ëª©:
# - STACK_NAME
# - KEY_PAIR_NAME
# - S3_BUCKET

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° ì„¤ì • ìƒì„±
source environment-variables.sh
envsubst < cluster-config.yaml.template > cluster-config.yaml
```

**âš ï¸ AWS Managed Prometheus ì‚¬ìš© ì‹œ ì¶”ê°€ ì„¤ì • í•„ìš”**

`MonitoringType=amp-only` ë˜ëŠ” `amp+amg`ë¥¼ ì„ íƒí•œ ê²½ìš°, ëª¨ë“  ë…¸ë“œì— AMP remote_write IAM Policyë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤:

```bash
# AMP Remote Write Policy ARN í™•ì¸
AMP_POLICY_ARN=$(aws cloudformation describe-stacks \
  --stack-name parallelcluster-infra \
  --region $REGION \
  --query 'Stacks[0].Outputs[?OutputKey==`AMPRemoteWritePolicyArn`].OutputValue' \
  --output text)

echo "AMP Policy ARN: $AMP_POLICY_ARN"

# cluster-config.yamlì— ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€
# HeadNode, LoginNodes, ComputeNodesì˜ Iam.AdditionalIamPoliciesì— ì¶”ê°€:
#   - Policy: arn:aws:iam::123456789012:policy/parallelcluster-infra-amp-remote-write
```

ë˜ëŠ” environment-variables.shì— ì¶”ê°€í•˜ì—¬ ìë™í™”:

```bash
# environment-variables.shì— ì¶”ê°€
export AMP_POLICY_ARN="arn:aws:iam::123456789012:policy/parallelcluster-infra-amp-remote-write"

# cluster-config.yaml.templateì—ì„œ ì‚¬ìš©
# Iam:
#   AdditionalIamPolicies:
#     - Policy: ${AMP_POLICY_ARN}
```

ğŸ“– **ìƒì„¸ ì„¤ì • ê°€ì´ë“œ**: [guide/CONFIGURATION.md](guide/CONFIGURATION.md)

### 4. í´ëŸ¬ìŠ¤í„° ìƒì„±

```bash
# í´ëŸ¬ìŠ¤í„° ìƒì„± (WaitCondition íƒ€ì„ì•„ì›ƒ ë°©ì§€ë¥¼ ìœ„í•´ ìµœì†Œ ì„¤ì¹˜ë§Œ ìˆ˜í–‰)
pcluster create-cluster \
  --cluster-name my-cluster \
  --cluster-configuration cluster-config.yaml

# ìƒì„± ìƒíƒœ í™•ì¸
pcluster describe-cluster --cluster-name my-cluster
```

### 5. ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ì¹˜

ì„¸ ê°€ì§€ ë°©ë²• ì¤‘ ì„ íƒí•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”:

#### ë°©ë²• 1: ê³µìœ  ìŠ¤í† ë¦¬ì§€ í™œìš© (ê¶Œì¥)

FSx Lustreì— í•œ ë²ˆë§Œ ì„¤ì¹˜í•˜ê³  ëª¨ë“  ë…¸ë“œì—ì„œ ì°¸ì¡°:

```bash
# HeadNodeì—ì„œ NCCL ì„¤ì¹˜ (í•œ ë²ˆë§Œ, 10-15ë¶„ ì†Œìš”)
ssh headnode
sudo bash /fsx/nccl/install-nccl-shared.sh v2.28.7-1 v1.17.2-aws /fsx
```

**ComputeNode ìë™ ê°ì§€**:
- âœ… **ìƒˆë¡œ ì‹œì‘ë˜ëŠ” ë…¸ë“œ**: ìë™ìœ¼ë¡œ `/fsx/nccl/setup-nccl-env.sh` ê°ì§€ ë° ì„¤ì •
- âš ï¸ **ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë…¸ë“œ**: ìˆ˜ë™ ì ìš© í•„ìš”

```bash
# ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ComputeNodeì— ì ìš© (í´ëŸ¬ìŠ¤í„° ìƒì„± í›„ NCCL ì„¤ì¹˜í•œ ê²½ìš°)
bash /fsx/nccl/apply-nccl-to-running-nodes.sh

# ë˜ëŠ” ìˆ˜ë™ìœ¼ë¡œ
srun --nodes=ALL bash -c 'cat > /etc/profile.d/nccl-shared.sh << "EOF"
source /fsx/nccl/setup-nccl-env.sh
EOF
chmod +x /etc/profile.d/nccl-shared.sh'
```

**ê¶Œì¥ ì›Œí¬í”Œë¡œìš°**:
1. í´ëŸ¬ìŠ¤í„° ìƒì„± (ComputeNode MinCount=0ìœ¼ë¡œ ì„¤ì •)
2. HeadNodeì—ì„œ NCCL ì„¤ì¹˜
3. Slurm job ì œì¶œ â†’ ComputeNode ìë™ ì‹œì‘ â†’ NCCL ìë™ ê°ì§€ âœ…

**ì¥ì **: 
- ë¹ ë¥¸ ì„¤ì¹˜ (10-15ë¶„, í•œ ë²ˆë§Œ)
- ìŠ¤í† ë¦¬ì§€ íš¨ìœ¨ (ëª¨ë“  ë…¸ë“œê°€ ê³µìœ )
- ë²„ì „ ì¼ê´€ì„±
- ìƒˆ ë…¸ë“œ ìë™ ê°ì§€

#### ë°©ë²• 2: í´ëŸ¬ìŠ¤í„° ìƒì„± ì‹œ ìë™ ì„¤ì¹˜

`cluster-config.yaml`ì˜ CustomActionsë¡œ ë¹ ë¥¸ ì„¤ì¹˜ ìë™í™”:

```yaml
ComputeResources:
  - Name: distributed-ml
    CustomActions:
      OnNodeConfigured:
        Script: s3://my-bucket/config/compute/install-pyxis.sh
```

**ì£¼ì˜**: NCCL ê°™ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ì‘ì—…(10-15ë¶„)ì€ WaitCondition íƒ€ì„ì•„ì›ƒ(30ë¶„)ì„ ìœ ë°œí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³„ë„ ì„¤ì¹˜ ê¶Œì¥

#### ë°©ë²• 3: ì»¨í…Œì´ë„ˆ ì‚¬ìš©

ì‚¬ì „ êµ¬ì„±ëœ ì»¨í…Œì´ë„ˆë¡œ ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ì¹˜ ë¶ˆí•„ìš”:

```bash
# Slurm jobì—ì„œ ì»¨í…Œì´ë„ˆ ì‹¤í–‰
srun --container-image=nvcr.io/nvidia/pytorch:24.01-py3 \
     --container-mounts=/fsx:/fsx \
     python /fsx/train.py
```

**ì¥ì **: ì„¤ì¹˜ ë¶ˆí•„ìš”, ì¬í˜„ ê°€ëŠ¥, ë²„ì „ ê´€ë¦¬ ìš©ì´

ğŸ“– **ìƒì„¸ ì„¤ì¹˜ ê°€ì´ë“œ**: [guide/INSTALLATION.md](guide/INSTALLATION.md)

### Bootstrap íƒ€ì„ì•„ì›ƒ ì„¤ì •

ParallelClusterëŠ” ë…¸ë“œ ì´ˆê¸°í™” ì‹œ CloudFormation WaitConditionì„ ì‚¬ìš©í•˜ë©°, ê¸°ë³¸ íƒ€ì„ì•„ì›ƒì€ 30ë¶„ì…ë‹ˆë‹¤. GPU ì¸ìŠ¤í„´ìŠ¤(íŠ¹íˆ p5en.48xlarge)ëŠ” EFA ë“œë¼ì´ë²„ì™€ NVIDIA ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ì¹˜ì— ì‹œê°„ì´ ë” ê±¸ë¦¬ë¯€ë¡œ íƒ€ì„ì•„ì›ƒì„ ëŠ˜ë ¤ì•¼ í•©ë‹ˆë‹¤.

**í˜„ì¬ ì„¤ì •** (`cluster-config.yaml`):

```yaml
DevSettings:
  Timeouts:
    HeadNodeBootstrapTimeout: 3600      # 60ë¶„
    ComputeNodeBootstrapTimeout: 2400   # 40ë¶„
```

**íƒ€ì„ì•„ì›ƒ ê·¼ê±°**:

| ë…¸ë“œ íƒ€ì… | ì‹¤ì œ ì„¤ì¹˜ ì‹œê°„ | íƒ€ì„ì•„ì›ƒ ì„¤ì • | ì•ˆì „ ë§ˆì§„ |
|-----------|----------------|---------------|-----------|
| **HeadNode** | ~5ë¶„ | 60ë¶„ | 12Ã— |
| **ComputeNode** | 15-20ë¶„ | 40ë¶„ | 2Ã— |

**ComputeNode ì„¤ì¹˜ ì‹œê°„ ìƒì„¸**:

```
EFA Driver:              5-10ë¶„  â† ê°€ì¥ ì˜¤ë˜ ê±¸ë¦¼
Docker + NVIDIA Toolkit:  3ë¶„
Pyxis:                    2ë¶„
CloudWatch Agent:         1ë¶„
DCGM Exporter:            1ë¶„
Node Exporter:            1ë¶„
NCCL ì„¤ì •:                5ì´ˆ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ì´ ì‹¤ì œ ì‹œê°„:            15-20ë¶„
íƒ€ì„ì•„ì›ƒ ì„¤ì •:            40ë¶„
ì•ˆì „ ë§ˆì§„:               20ë¶„
```

**íƒ€ì„ì•„ì›ƒ ì¦ìƒ**:
- ComputeNodeê°€ `running` ìƒíƒœì—ì„œ ê³§ë°”ë¡œ `shutting-down`ìœ¼ë¡œ ì „í™˜
- CloudWatch ë¡œê·¸ì—ì„œ ì„¤ì¹˜ê°€ ì¤‘ê°„ì— ì¤‘ë‹¨ë¨
- CloudFormation ì´ë²¤íŠ¸ì— "timeout" ë©”ì‹œì§€

**íƒ€ì„ì•„ì›ƒ ì¡°ì •ì´ í•„ìš”í•œ ê²½ìš°**:
- âœ… ëŠë¦° ë„¤íŠ¸ì›Œí¬ í™˜ê²½
- âœ… ëŒ€í˜• ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… (ë” ë§ì€ ë“œë¼ì´ë²„ ì„¤ì¹˜)
- âœ… ë³µì¡í•œ CustomActions ìŠ¤í¬ë¦½íŠ¸
- âœ… ì¶”ê°€ ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ì¹˜

**íƒ€ì„ì•„ì›ƒ ëª¨ë‹ˆí„°ë§**:

```bash
# CloudFormation ì´ë²¤íŠ¸ í™•ì¸
aws cloudformation describe-stack-events \
  --stack-name p5en-48xlarge-cluster \
  --region us-east-2 \
  --query 'StackEvents[?contains(ResourceStatusReason, `timeout`)]'

# ì¸ìŠ¤í„´ìŠ¤ ìƒíƒœ í™•ì¸
aws ec2 describe-instances \
  --filters "Name=tag:aws:cloudformation:stack-name,Values=p5en-48xlarge-cluster" \
  --region us-east-2 \
  --query 'Reservations[*].Instances[*].{ID:InstanceId,State:State.Name,LaunchTime:LaunchTime}'

# CloudWatch ë¡œê·¸ í™•ì¸
aws logs tail /aws/parallelcluster/p5en-48xlarge-cluster --region us-east-2 --since 1h
```

ğŸ“– **íƒ€ì„ì•„ì›ƒ ìƒì„¸ ê°€ì´ë“œ**: [guide/TIMEOUT-CONFIGURATION.md](guide/TIMEOUT-CONFIGURATION.md)

### 6. ëª¨ë‹ˆí„°ë§ ì ‘ê·¼

#### Option 1: Amazon Managed Grafana (ê¶Œì¥)

```bash
# Grafana ì ‘ì† ì •ë³´ í™•ì¸ (amp+amg ì˜µì…˜ ì‚¬ìš© ì‹œ)
aws cloudformation describe-stacks \
  --stack-name parallelcluster-infra \
  --query 'Stacks[0].Outputs[?OutputKey==`GrafanaAccessInstructions`].OutputValue' \
  --output text

# ë˜ëŠ” URLë§Œ í™•ì¸
GRAFANA_URL=$(aws cloudformation describe-stacks \
  --stack-name parallelcluster-infra \
  --query 'Stacks[0].Outputs[?OutputKey==`ManagedGrafanaWorkspaceEndpoint`].OutputValue' \
  --output text)

echo "Grafana: https://${GRAFANA_URL}"
# AWS SSOë¡œ ë¡œê·¸ì¸ (ê¶Œí•œ ë¶€ì—¬ í›„)
```

#### Option 2: Self-hosting (ALB)

```bash
# ALB DNS í™•ì¸
aws cloudformation describe-stacks \
  --stack-name parallelcluster-infra \
  --query 'Stacks[0].Outputs[?OutputKey==`ALBDNSName`].OutputValue' \
  --output text

# ì ‘ì†: https://<ALB-DNS>/grafana/
# ê¸°ë³¸ ë¡œê·¸ì¸: admin / Grafana4PC!
```

ğŸ“– **ëª¨ë‹ˆí„°ë§ ì„¤ì • ê°€ì´ë“œ**: [guide/MONITORING.md](guide/MONITORING.md)

### 7. NCCL ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

```bash
# í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ë³µì‚¬
cp -r tests/nccl/ /fsx/nccl-tests/

# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
sbatch /fsx/nccl-tests/nccl-benchmark-suite.sbatch

# ì‘ì—… ìƒíƒœ í™•ì¸
squeue
```

## ğŸ“¡ Monitoring

### í†µí•© ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ

ì´ ì†”ë£¨ì…˜ì€ GPU, ì‹œìŠ¤í…œ, ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥ì„ í¬ê´„í•˜ëŠ” ì™„ì „í•œ ëª¨ë‹ˆí„°ë§ ìŠ¤íƒì„ ì œê³µí•©ë‹ˆë‹¤:

| ëª¨ë‹ˆí„°ë§ ì˜ì—­ | ë„êµ¬ | ë©”íŠ¸ë¦­ | í¬íŠ¸ |
|--------------|------|--------|------|
| **GPU ì„±ëŠ¥** | DCGM Exporter | GPU ì‚¬ìš©ë¥ , ë©”ëª¨ë¦¬, ì˜¨ë„, ì „ë ¥ | 9400 |
| **NVLink** | DCGM | GPU ê°„ í†µì‹  ëŒ€ì—­í­ | - |
| **EFA ë„¤íŠ¸ì›Œí¬** | EFA Monitor | ë…¸ë“œ ê°„ ë„¤íŠ¸ì›Œí¬ ì²˜ë¦¬ëŸ‰, íŒ¨í‚· ì†ë„ | - |
| **ì‹œìŠ¤í…œ** | Node Exporter | CPU, ë©”ëª¨ë¦¬, ë””ìŠ¤í¬ | 9100 |
| **Slurm** | Custom Collector | ì‘ì—… í, ë…¸ë“œ ìƒíƒœ | - |

### ìë™ ì„¤ì¹˜

ëª¨ë“  ëª¨ë‹ˆí„°ë§ ì»´í¬ë„ŒíŠ¸ëŠ” í´ëŸ¬ìŠ¤í„° ìƒì„± ì‹œ ìë™ìœ¼ë¡œ ì„¤ì¹˜ë©ë‹ˆë‹¤:

- **HeadNode**: Prometheus (ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì €ì¥)
- **ComputeNode (GPU)**: DCGM Exporter + Node Exporter + EFA Monitor
- **ComputeNode (CPU)**: Node Exporterë§Œ ì„¤ì¹˜

### ëª¨ë‹ˆí„°ë§ ê°€ì´ë“œ

- [CloudWatch ëª¨ë‹ˆí„°ë§](guide/MONITORING.md) - ê¸°ë³¸ ëª¨ë‹ˆí„°ë§ ì„¤ì •
- [DCGM GPU ëª¨ë‹ˆí„°ë§](guide/DCGM-TO-CLOUDWATCH.md) - GPU ë©”íŠ¸ë¦­ ìƒì„¸
- [NVLink ëª¨ë‹ˆí„°ë§](guide/NVLINK-MONITORING.md) - GPU ê°„ í†µì‹ 
- [EFA ë„¤íŠ¸ì›Œí¬ ëª¨ë‹ˆí„°ë§](guide/EFA-MONITORING.md) - ë…¸ë“œ ê°„ ë„¤íŠ¸ì›Œí¬
- [Prometheus ë©”íŠ¸ë¦­](guide/PROMETHEUS-METRICS.md) - ë©”íŠ¸ë¦­ ì¿¼ë¦¬ ê°€ì´ë“œ
- [AMP + AMG ì„¤ì •](guide/AMP-AMG-SETUP.md) - AWS ê´€ë¦¬í˜• ëª¨ë‹ˆí„°ë§

### ëŒ€ì‹œë³´ë“œ ì ‘ê·¼

```bash
# CloudWatch ëŒ€ì‹œë³´ë“œ (ìë™ ìƒì„±)
# - ParallelCluster-<cluster-name>: ê¸°ë³¸ ëŒ€ì‹œë³´ë“œ
# - ParallelCluster-<cluster-name>-Advanced: ê³ ê¸‰ ë©”íŠ¸ë¦­
# - ParallelCluster-<cluster-name>-EFA: EFA ë„¤íŠ¸ì›Œí¬

# Grafana (self-hosting ë˜ëŠ” AMG)
# - GPU Performance
# - NVLink Bandwidth
# - EFA Network
# - Slurm Jobs
```

## ğŸ”§ ì£¼ìš” ì„¤ì •

### Capacity Blockê³¼ Placement Group

> âš ï¸ **ì¤‘ìš”**: Capacity Blockê³¼ Placement Groupì€ ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**Capacity Block ì‚¬ìš© ì‹œ**:
- `cluster-config.yaml`ì—ì„œ `PlacementGroup.Enabled: false` ì„¤ì • í•„ìˆ˜
- Single Spine êµ¬ì„±ì´ í•„ìš”í•œ ê²½ìš° Capacity Block ì˜ˆì•½ ì „ AWS Account Teamì— ë¬¸ì˜
- í† í´ë¡œì§€ í™•ì¸: [EC2 Instance Topology](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-topology.html)

**On-Demand/Spot ì‚¬ìš© ì‹œ**:
- Placement Group í™œì„±í™” ê¶Œì¥ (ìµœì ì˜ ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥)

ğŸ“– **ìƒì„¸ ê°€ì´ë“œ**: [guide/CONFIGURATION.md](guide/CONFIGURATION.md#ï¸-capacity-blockê³¼-placement-group-ì œì•½ì‚¬í•­)

### ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ì„ íƒ

**HeadNodeì™€ LoginNodeëŠ” GPUê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤** - ë¹„ìš© ìµœì í™”ë¥¼ ìœ„í•´ CPU ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.

| ë…¸ë“œ íƒ€ì… | ê¶Œì¥ ì¸ìŠ¤í„´ìŠ¤ | ìš©ë„ | ë¹„ìš© ì ˆê° |
|-----------|---------------|------|-----------|
| HeadNode | m5.2xlarge ~ m5.8xlarge | Slurm ìŠ¤ì¼€ì¤„ëŸ¬ | ~99% |
| LoginNode | m5.large ~ m5.2xlarge | ì‚¬ìš©ì ì ‘ê·¼, ì „ì²˜ë¦¬ | ~99% |
| ComputeNode | p6-b200.48xlarge | GPU ì›Œí¬ë¡œë“œ | - |
| Monitoring | t3.medium | ëª¨ë‹ˆí„°ë§ ì „ìš© | - |

### ìŠ¤í† ë¦¬ì§€ êµ¬ì„±

- **FSx Lustre** (`/fsx`): ê³ ì„±ëŠ¥ ê³µìœ  ìŠ¤í† ë¦¬ì§€
  - ë°ì´í„°ì…‹, ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸, í•™ìŠµ ì¶œë ¥
  - ë©€í‹° GB/s ì²˜ë¦¬ëŸ‰
  
- **HeadNode NFS** (`/home`): ê¸°ë³¸ ê³µìœ  ë””ë ‰í† ë¦¬
  - ì‚¬ìš©ì íŒŒì¼, ìŠ¤í¬ë¦½íŠ¸
  - ì¶”ê°€ ë¹„ìš© ì—†ìŒ
  
- **EBS**: ë£¨íŠ¸ ë³¼ë¥¨ ë° ë¡œì»¬ ìŠ¤í¬ë˜ì¹˜

### WaitCondition íƒ€ì„ì•„ì›ƒ ê´€ë¦¬

ParallelClusterëŠ” ë…¸ë“œ ë°°í¬ ì‹œ 30ë¶„ WaitCondition ì œí•œì´ ìˆìŠµë‹ˆë‹¤.

**ê¶Œì¥ ì „ëµ**:
1. âœ… **í´ëŸ¬ìŠ¤í„° ìƒì„± ì‹œ**: ìµœì†Œ ì„¤ì¹˜ë§Œ ìˆ˜í–‰ (ë¹ ë¥¸ ë°°í¬)
2. âœ… **ìƒì„± ì™„ë£Œ í›„**: í•„ìš”í•œ ì†Œí”„íŠ¸ì›¨ì–´ ìˆ˜ë™ ì„¤ì¹˜
3. âœ… **ê³µìœ  ìŠ¤í† ë¦¬ì§€ í™œìš©**: í•œ ë²ˆ ì„¤ì¹˜í•˜ì—¬ ëª¨ë“  ë…¸ë“œì—ì„œ ì°¸ì¡°
4. âœ… **ì»¨í…Œì´ë„ˆ ì‚¬ìš©**: ì‚¬ì „ êµ¬ì„±ëœ ì´ë¯¸ì§€ í™œìš©

**ë‹¤ìˆ˜ì˜ ComputeNode ê´€ë¦¬**:
- ê°œë³„ SSH ì ‘ì† ëŒ€ì‹  Slurm jobìœ¼ë¡œ ì¼ê´„ ì„¤ì¹˜
- ê³µìœ  ìŠ¤í† ë¦¬ì§€ì— ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ì¹˜ í›„ ì°¸ì¡°
- Docker/Singularity ì»¨í…Œì´ë„ˆ ì‚¬ìš©

ğŸ“– **ìƒì„¸ ê°€ì´ë“œ**: [guide/INSTALLATION.md](guide/INSTALLATION.md)

## ğŸ“Š Expected Performance

### p6-b200.48xlarge ì‚¬ì–‘

| í•­ëª© | ì‚¬ì–‘ |
|------|------|
| vCPUs | 192 |
| Memory | 2,048 GiB (2TB DDR5) |
| GPUs | 8x NVIDIA B200 (192GB HBM3e each) |
| Network | 3,200 Gbps |
| NVLink | 900 GB/s per direction |
| Storage | 8x 3.84TB NVMe SSD |

### ì„±ëŠ¥ ì§€í‘œ

- **ë‹¨ì¼ ë…¸ë“œ**: 1.2-1.4 TB/s NCCL ëŒ€ì—­í­
- **ë©€í‹° ë…¸ë“œ**: >90% í™•ì¥ íš¨ìœ¨ì„±
- **ë„¤íŠ¸ì›Œí¬ ì§€ì—°**: 2-5Î¼s (inter-node)

## ğŸ›¡ï¸ Security

### ë³´ì•ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] SSH ì ‘ê·¼ì„ íŠ¹ì • IPë¡œ ì œí•œ (`AllowedIPsForLoginNodeSSH`)
- [ ] Monitoring InstanceëŠ” ALBë¥¼ í†µí•´ì„œë§Œ ì ‘ê·¼
- [ ] Grafana ê¸°ë³¸ ë¹„ë°€ë²ˆí˜¸ ë³€ê²½
- [ ] SSM Session Manager ì‚¬ìš© (SSH ëŒ€ì‹ )
- [ ] HeadNode/ComputeNodeëŠ” Private Subnetì— ë°°ì¹˜

### ì•ˆì „í•œ ì ‘ê·¼ ë°©ë²•

```bash
# SSM Session Manager (ê¶Œì¥)
aws ssm start-session --target <Instance-ID>

# Grafana í¬íŠ¸ í¬ì›Œë”©
aws ssm start-session \
  --target <Monitoring-Instance-ID> \
  --document-name AWS-StartPortForwardingSession \
  --parameters '{"portNumber":["3000"],"localPortNumber":["3000"]}'
```

ğŸ“– **ë³´ì•ˆ ê°€ì´ë“œ**: [guide/SECURITY.md](guide/SECURITY.md)

## ğŸ” Troubleshooting

ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²°ì€ [guide/TROUBLESHOOTING.md](guide/TROUBLESHOOTING.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

**ë¹ ë¥¸ ë¬¸ì œ í•´ê²°**:

```bash
# í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
pcluster describe-cluster --cluster-name my-cluster

# ë¡œê·¸ í™•ì¸
pcluster get-cluster-log-events --cluster-name my-cluster

# ì„¤ì • ê²€ì¦
pcluster validate-cluster-configuration --cluster-configuration cluster-config.yaml
```

## ğŸ“š Additional Resources

- [AWS ParallelCluster User Guide](https://docs.aws.amazon.com/parallelcluster/)
- [NVIDIA B200 Documentation](https://www.nvidia.com/en-us/data-center/b200/)
- [NCCL Developer Guide](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/)
- [EFA User Guide](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html)

## ğŸ“„ License

This project is licensed under the MIT-0 License.

## ğŸ·ï¸ Tags

`aws` `parallelcluster` `p6` `b200` `gpu` `hpc` `machine-learning` `nccl` `slurm` `efa`
